link = entry["link"]
# Check if the URL already exists
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)  # Update existing URLs
# Write new entries to a temporary file
temp_file = "temp_publications.qmd"
with open(temp_file, "w", encoding='utf-8') as temp_out:
# Write new entries
for entry in new_entries:
temp_out.write(entry)
# Append existing content after the new entries
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
temp_out.write(existing_file.read())
# Replace the original file with the temporary file
os.replace(temp_file, "publications.qmd")
# Print the first existing URL and the first new URL
print("First existing URL:", next(iter(existing_urls), "No existing URLs"))
print("First new URL:", first_new_url if first_new_url is not None else "No new URLs")
import feedparser
import os
# Parse the RSS feed
NewsFeed = feedparser.parse("https://pubmed.ncbi.nlm.nih.gov/rss/search/1jue2dFuI_LFj_phV9s9PjTEhPInBPr1NTb858Yzvn1De2GMXv/?limit=5000&utm_campaign=pubmed-2&fc=20230630074326")
# Collect existing URLs from the existing content
existing_urls = set()
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
for line in existing_file:
if line.strip().startswith("https://"):
existing_urls.add(line.strip())
# Collect new entries that do not already exist
new_entries = []
first_new_url = None
for entry in NewsFeed.entries:
link = entry["link"]
# Check if the URL already exists
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)  # Update existing URLs
# Write new entries to a temporary file
temp_file = "temp_publications.qmd"
with open(temp_file, "w", encoding='utf-8') as temp_out:
# Write new entries
for entry in new_entries:
temp_out.write(entry)
# Append existing content after the new entries
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
temp_out.write(existing_file.read())
# Replace the original file with the temporary file
os.replace(temp_file, "publications.qmd")
# Print the first existing URL and the first new URL
print("First existing URL:", next(iter(existing_urls), "No existing URLs"))
print("First new URL:", first_new_url if first_new_url is not None else "No new URLs")
import feedparser
import os
# Parse the RSS feed
NewsFeed = feedparser.parse("https://pubmed.ncbi.nlm.nih.gov/rss/search/1jue2dFuI_LFj_phV9s9PjTEhPInBPr1NTb858Yzvn1De2GMXv/?limit=5000&utm_campaign=pubmed-2&fc=20230630074326")
# Collect existing URLs from the existing content
existing_urls = set()
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
for line in existing_file:
if line.strip().startswith("https://"):
existing_urls.add(line.strip())
# Collect new entries that do not already exist
new_entries = []
first_new_url = None
for entry in NewsFeed.entries:
link = entry["link"]
# Check if the URL already exists
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)  # Update existing URLs
# Write new entries to a temporary file
temp_file = "temp_publications.qmd"
with open(temp_file, "w", encoding='utf-8') as temp_out:
# Truncate the file to ensure it's empty before writing
temp_out.truncate(0)
# Write new entries
for entry in new_entries:
temp_out.write(entry)
# Append existing content after the new entries
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
temp_out.write(existing_file.read())
# Replace the original file with the temporary file
os.replace(temp_file, "publications.qmd")
# Print the first existing URL and the first new URL
print("First existing URL:", next(iter(existing_urls), "No existing URLs"))
print("First new URL:", first_new_url if first_new_url is not None else "No new URLs")
print(exiting_urls)
print(existing_urls)
print(link)
link in existing_urls
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)
# Write new entries
for entry in new_entries:
entry["dc_identifier"]
print(entry["dc_identifier"])
print(entry)
print(entry)
print(locals())
del locals()
l = locals()
del l
print(entry)
del entry
print(entry)
import feedparser
import os
# Parse the RSS feed
NewsFeed = feedparser.parse("https://pubmed.ncbi.nlm.nih.gov/rss/search/1jue2dFuI_LFj_phV9s9PjTEhPInBPr1NTb858Yzvn1De2GMXv/?limit=5000&utm_campaign=pubmed-2&fc=20230630074326")
if 'new_entries' in locals():
del new_entries
if 'entry' in locals():
del entry
# Collect existing URLs from the existing content
existing_urls = set()
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
for line in existing_file:
if line.strip().startswith("https://"):
existing_urls.add(line.strip())
# Collect new entries that do not already exist
new_entries = []
first_new_url = None
for entry in NewsFeed.entries:
link = entry["link"]
# Check if the URL already exists
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)  # Update existing URLs
# Write new entries to a temporary file
temp_file = "temp_publications.qmd"
with open(temp_file, "w", encoding='utf-8') as temp_out:
# Truncate the file to ensure it's empty before writing
temp_out.truncate(0)
# Write new entries
for entry in new_entries:
temp_out.write(entry)
# Append existing content after the new entries
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
temp_out.write(existing_file.read())
# Replace the original file with the temporary file
os.replace(temp_file, "publications.qmd")
# Print the first existing URL and the first new URL
print("First existing URL:", next(iter(existing_urls), "No existing URLs"))
print("First new URL:", first_new_url if first_new_url is not None else "No new URLs")
import feedparser
import os
# Parse the RSS feed
NewsFeed = feedparser.parse("https://pubmed.ncbi.nlm.nih.gov/rss/search/1jue2dFuI_LFj_phV9s9PjTEhPInBPr1NTb858Yzvn1De2GMXv/?limit=5000&utm_campaign=pubmed-2&fc=20230630074326")
if 'new_entries' in locals():
del new_entries
if 'entry' in locals():
del entry
if 'temp_out' in locals():
del temp_out
# Collect existing URLs from the existing content
existing_urls = set()
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
for line in existing_file:
if line.strip().startswith("https://"):
existing_urls.add(line.strip())
# Collect new entries that do not already exist
new_entries = []
first_new_url = None
for entry in NewsFeed.entries:
link = entry["link"]
# Check if the URL already exists
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)  # Update existing URLs
# Write new entries to a temporary file
temp_file = "temp_publications.qmd"
with open(temp_file, "w", encoding='utf-8') as temp_out:
# Truncate the file to ensure it's empty before writing
temp_out.truncate(0)
# Write new entries
for entry in new_entries:
temp_out.write(entry)
# Append existing content after the new entries
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
temp_out.write(existing_file.read())
# Replace the original file with the temporary file
os.replace(temp_file, "publications.qmd")
# Print the first existing URL and the first new URL
print("First existing URL:", next(iter(existing_urls), "No existing URLs"))
print("First new URL:", first_new_url if first_new_url is not None else "No new URLs")
import feedparser
import os
# Parse the RSS feed
NewsFeed = feedparser.parse("https://pubmed.ncbi.nlm.nih.gov/rss/search/1jue2dFuI_LFj_phV9s9PjTEhPInBPr1NTb858Yzvn1De2GMXv/?limit=5000&utm_campaign=pubmed-2&fc=20230630074326")
if 'new_entries' in locals():
del new_entries
if 'entry' in locals():
del entry
if 'temp_out' in locals():
del temp_out
# Collect existing URLs from the existing content
existing_urls = set()
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
for line in existing_file:
if line.strip().startswith("https://"):
existing_urls.add(line.strip())
# Collect new entries that do not already exist
new_entries = []
first_new_url = None
for entry in NewsFeed.entries:
link = entry["link"]
# Check if the URL already exists
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)  # Update existing URLs
# Write new entries to a temporary file
temp_file = "temp_publications.qmd"
with open(temp_file, "w", encoding='utf-8') as temp_out:
# Truncate the file to ensure it's empty before writing
temp_out.truncate(0)
# Write new entries
for entry in new_entries:
temp_out.write(entry)
# Append existing content after the new entries
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
temp_out.write(existing_file.read())
# Replace the original file with the temporary file
os.replace(temp_file, "publications.qmd")
# Print the first existing URL and the first new URL
print("First existing URL:", next(iter(existing_urls), "No existing URLs"))
print("First new URL:", first_new_url if first_new_url is not None else "No new URLs")
l = locals()
del l
del l
all_variables = locals()
# Iterate over all variable names and delete them
for var_name in list(all_variables.keys()):
# Exclude special variables starting with '__' (e.g., '__name__', '__doc__')
if not var_name.startswith('__'):
del all_variables[var_name]
import feedparser
import os
# Get a dictionary containing all variables in the current namespace
all_variables = locals()
# Iterate over all variable names and delete them
for var_name in list(all_variables.keys()):
# Exclude special variables starting with '__' (e.g., '__name__', '__doc__')
if not var_name.startswith('__'):
del all_variables[var_name]
# Parse the RSS feed
NewsFeed = feedparser.parse("https://pubmed.ncbi.nlm.nih.gov/rss/search/1jue2dFuI_LFj_phV9s9PjTEhPInBPr1NTb858Yzvn1De2GMXv/?limit=5000&utm_campaign=pubmed-2&fc=20230630074326")
# Collect existing URLs from the existing content
existing_urls = set()
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
for line in existing_file:
if line.strip().startswith("https://"):
existing_urls.add(line.strip())
# Collect new entries that do not already exist
new_entries = []
first_new_url = None
for entry in NewsFeed.entries:
link = entry["link"]
# Check if the URL already exists
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)  # Update existing URLs
# Write new entries to a temporary file
temp_file = "temp_publications.qmd"
with open(temp_file, "w", encoding='utf-8') as temp_out:
# Truncate the file to ensure it's empty before writing
temp_out.truncate(0)
# Write new entries
for entry in new_entries:
temp_out.write(entry)
# Append existing content after the new entries
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
temp_out.write(existing_file.read())
# Replace the original file with the temporary file
os.replace(temp_file, "publications.qmd")
# Print the first existing URL and the first new URL
print("First existing URL:", next(iter(existing_urls), "No existing URLs"))
print("First new URL:", first_new_url if first_new_url is not None else "No new URLs")
# Get a dictionary containing all variables in the current namespace
all_variables = locals()
# Iterate over all variable names and delete them
for var_name in list(all_variables.keys()):
# Exclude special variables starting with '__' (e.g., '__name__', '__doc__')
if not var_name.startswith('__'):
del all_variables[var_name]
import feedparser
import os
# Parse the RSS feed
NewsFeed = feedparser.parse("https://pubmed.ncbi.nlm.nih.gov/rss/search/1jue2dFuI_LFj_phV9s9PjTEhPInBPr1NTb858Yzvn1De2GMXv/?limit=5000&utm_campaign=pubmed-2&fc=20230630074326")
# Collect existing URLs from the existing content
existing_urls = set()
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
for line in existing_file:
if line.strip().startswith("https://"):
existing_urls.add(line.strip())
# Collect new entries that do not already exist
new_entries = []
first_new_url = None
for entry in NewsFeed.entries:
link = entry["link"]
# Check if the URL already exists
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)  # Update existing URLs
# Write new entries to a temporary file
temp_file = "temp_publications.qmd"
with open(temp_file, "w", encoding='utf-8') as temp_out:
# Truncate the file to ensure it's empty before writing
temp_out.truncate(0)
# Write new entries
for entry in new_entries:
temp_out.write(entry)
# Append existing content after the new entries
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
temp_out.write(existing_file.read())
# Replace the original file with the temporary file
os.replace(temp_file, "publications.qmd")
# Print the first existing URL and the first new URL
print("First existing URL:", next(iter(existing_urls), "No existing URLs"))
print("First new URL:", first_new_url if first_new_url is not None else "No new URLs")
# Get a dictionary containing all variables in the current namespace
all_variables = locals()
# Iterate over all variable names and delete them
for var_name in list(all_variables.keys()):
# Exclude special variables starting with '__' (e.g., '__name__', '__doc__')
if not var_name.startswith('__'):
del all_variables[var_name]
import feedparser
import os
# Parse the RSS feed
NewsFeed = feedparser.parse("https://pubmed.ncbi.nlm.nih.gov/rss/search/1jue2dFuI_LFj_phV9s9PjTEhPInBPr1NTb858Yzvn1De2GMXv/?limit=5000&utm_campaign=pubmed-2&fc=20230630074326")
# Collect existing URLs from the existing content
existing_urls = set()
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
for line in existing_file:
if line.strip().startswith("https://"):
existing_urls.add(line.strip())
# Collect new entries that do not already exist
new_entries = []
first_new_url = None
for entry in NewsFeed.entries:
link = entry["link"]
# Check if the URL already exists
if link not in existing_urls:
if first_new_url is None:
first_new_url = link  # Store the first new URL
author_list = []
title = entry["title"]
publish_time = entry["published"].split(" ")
time_string = publish_time[1] + " " + publish_time[2] + " " + publish_time[3]
paper_string = entry["dc_source"] + ", " + entry["dc_identifier"] + ", " + time_string
if "authors" in entry and isinstance(entry["authors"], list):
for i in entry["authors"]:
author_list.append(i["name"])
author_string = ", ".join(author_list)
else:
author_string = "No authors listed"
out_string = "  \n" + author_string + "  \n" + title + "  \n" + paper_string + "  \n" + link + "  \n"
new_entries.append(out_string)
existing_urls.add(link)  # Update existing URLs
# Write new entries to a temporary file
temp_file = "temp_publications.qmd"
with open(temp_file, "w", encoding='utf-8') as temp_out:
# Truncate the file to ensure it's empty before writing
temp_out.truncate(0)
# Write new entries
for entry in new_entries:
temp_out.write(entry)
# Append existing content after the new entries
with open("publications.qmd", "r", encoding='utf-8') as existing_file:
temp_out.write(existing_file.read())
# Replace the original file with the temporary file
os.replace(temp_file, "publications.qmd")
# Print the first existing URL and the first new URL
print("First existing URL:", next(iter(existing_urls), "No existing URLs"))
print("First new URL:", first_new_url if first_new_url is not None else "No new URLs")
library(reticulate)
use_python("/Applications/anaconda3/bin/python")
library(reticulate)
use_python("/Applications/anaconda3/bin/python")
#py_install("feedparser")
py_install("pickle")
#py_install("feedparser")
#py_install("pickle")
reticulate::repl_python()
